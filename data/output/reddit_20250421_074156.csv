id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1k3qw05,You can become a millionaire working in Data,,1440,44,IdlePerfectionist,1745167815.0,https://i.redd.it/tk3ipjstr0we1.png,0.96,False,False,False,False
1k3vz1m,Anybody else find dbt documentation hopelessly confusing,"I have been using dbt for over 1 year now i moved to a new company and while there is a lot of documentation for DBT, what I have found is that it's not particularly well laid out unlike documentation for many python packages like pandas, for example, where you can go to a particular section and get an exhaustive list of all the options available to you.

 I find that Google is often the best way to parse my way through DBT documentation. It's not clear where to go to find an exhaustive list of all the options for yml files is so I keep stumbling across new things in dbt which shouldn't be the case. I should be able to read through documentation and find an exhaustive list of everything I need does anybody else find this to be the case? Or have any tips",28,4,yanicklloyd,1745181598.0,https://www.reddit.com/r/dataengineering/comments/1k3vz1m/anybody_else_find_dbt_documentation_hopelessly/,0.92,False,False,False,False
1k3pifk,Best tools for automation?,"I‚Äôve been tasked at work with automating some processes ‚Äî things like scraping data from emails with attached CSV files, or running a script that currently takes a couple of hours every few days.

I‚Äôm seeing this as a great opportunity to dive into some new tools and best practices, especially with a long-term goal of becoming a Data Engineer. That said, I‚Äôm not totally sure where to start, especially when it comes to automating multi-step processes ‚Äî like pulling data from an email or an API, processing it, and maybe loading it somewhere maybe like a PowerBi Dashbaord or Excel.

I‚Äôd really appreciate any recommendations on tools, workflows, or general approaches that could help with automation in this kind of context!",20,19,JeffTheSpider,1745164101.0,https://www.reddit.com/r/dataengineering/comments/1k3pifk/best_tools_for_automation/,0.92,False,False,False,False
1k41nsf,When is it ok to use any non ACID compliant db ?,"I don‚Äôt understand when anyone would use a non acid compliant DB. Like I understand that they are very fast can deliver a lot of data and xyz but why is it worth it and how do you make it work ?

Like is it by a second validation steps ? Instead of just writing the data all of your process write, then wait to validate if the data is store somewhere ? 

Like is it because the data itself isn‚Äôt valuable enough that even if you lost the data from one transaction it doesn‚Äôt matter ?

Like I know most social platforms use non acid compliant DB like Cassandra for example. But what happen under the hood ? Let‚Äôs say a user post something on the platform, it doesn‚Äôt just crash or say ‚Äúsent‚Äù and then it‚Äôs maybe not. Are there process to ensure that if something goes wrong the app handles it or this because this doesn‚Äôt happen very often nobody care ? Like the use will repost it‚Äôs thing if it didn‚Äôt work
Is the user or process alerted in such case and how ?

For example if this happen every 500 millions inserts and I have 500 billions records how could I even trust my data ? 

So yeah a lot of scattered question but I think the general idea is shared.",16,15,Commercial_Dig2401,1745198415.0,https://www.reddit.com/r/dataengineering/comments/1k41nsf/when_is_it_ok_to_use_any_non_acid_compliant_db/,1.0,False,False,False,False
1k3u48f,Real-time 4/20 cannabis sales dashboard using streaming data,"We built this dashboard to visualize cannabis sales in real time across North America during 4/20. The data updates live from thousands of dispensary POS transactions as the day unfolds.

Under the hood, we‚Äôre using Estuary for data streaming and Tinybird to power super fast analytical queries. The charts are made in Tremor and the map is D3.",17,2,dabasaurus_rex_rawr,1745176381.0,https://420.headset.io,0.81,False,False,False,False
1k3rtpa,Best way to sync RDS Posgtres Full load + CDC data?,"What would this data pipeline look like? The total data size is 5TB on postgres and it is for a typical SaaS B2B2C product

Here is what the part of the data pipeline looks like

1. Source DB: Postgres running on RDS
2. AWS Database migration service -> Streams parquet into a s3 bucket
3. We have also exported the full db data into a different s3 bucket - this time almost matches the CDC start time

What we need on the other end is a good cost effective data lake to do analytics and reporting on - as real time as possible

I tried to set something up with pyiceberg to go iceberg -

\- Iceberg tables mirror the schema of posgtres tables

\- Each table is partitioned by account\_id and created\_date

I was able to load the full data easily but handling the CDC data is a challenge as the updates are damn slow. It feels impractical now - I am not sure if I should just append data to iceberg and get the latest row version by some other technique?

how is this typically done? Copy on write or merge on read?

What other ways of doing something like this exist that can work with 5TB data with 100GB data changes every day?",15,3,CityYogi,1745170273.0,https://www.reddit.com/r/dataengineering/comments/1k3rtpa/best_way_to_sync_rds_posgtres_full_load_cdc_data/,0.91,False,1745171357.0,False,False
1k3m7d4,Advice wanted: planning a Streamlit‚ÄØ+‚ÄØDuckDB geospatial app on Azure (Web‚ÄØApp‚ÄØService‚ÄØ+‚ÄØFunction),"Hey all,

I‚Äôm in the design phase for a lightweight, map‚Äëcentric web app and would love a sanity check before I start provisioning Azure resources.

Proposed architecture:
- Front‚Äëend: Streamlit container in an Azure Web‚ÄØApp Service. It plots store/parking locations on a Leaflet/folium map.
- Back‚Äëend: FastAPI wrapped in an Azure Function (Linux custom container). DuckDB runs inside the function.
- Data: A ~200‚ÄØMB GeoParquet file in Azure Blob Storage (hot tier).
- Networking: Web‚ÄØApp ‚Üî‚ÄØFunction over VNet integration and Private Endpoints; nothing goes out to the public internet.
- Data flow: User input ‚Üí Web‚ÄØApp calls /locations ‚Üí Function queries DuckDB ‚Üí returns payloads.

Open questions

	1.	Function vs. always‚Äëon container: Is a serverless Azure Function the right choice, or would something like Azure Container Apps (kept warm) be simpler for DuckDB workloads? Cold‚Äëstart worries me a bit.

	2.	Payload format: For ‚â§‚ÄØ200‚ÄØk rows, is it worth the complexity of sending Arrow/Polars over HTTP, or should I stick with plain JSON for map markers? Any real‚Äëworld gains?

	3.	Pre‚Äëprocessing beyond ‚Äúquery from Blob‚Äù: I might need server‚Äëside clustering, hexbin aggregation, or even vector‚Äëtile generation to keep the payload tiny. Where would you put that logic‚Äîinside the Function, a separate batch job, or something else?

	4.	Gotchas: Security, cost surprises, deployment quirks? Anything you wish you‚Äôd known before launching a similar setup?

Really appreciate any pointers, war stories, or blog posts you can share. üôè",13,4,Appropriate-Lab-Coat,1745154591.0,https://www.reddit.com/r/dataengineering/comments/1k3m7d4/advice_wanted_planning_a_streamlit_duckdb/,0.94,False,1745156033.0,False,False
1k408vu,Seeking Advice - Is DE at Meta worth pursuing?,"Hello fellow DEs!

I‚Äôm hoping to get some career advice from the experienced folks in this sub.

I have 4.5 YOE and a related master‚Äôs degree. Most of my experience has been in DE consulting, but earlier this year I grew tired of the consulting grind and began looking for something new. I applied to a bunch of roles, including a few at Meta, but never made it past initial screenings.

Fast forward to now ‚Äî I landed a senior DE position at a well-known crypto exchange about 4 months ago. I‚Äôm enjoying it so far: I‚Äôve been given a lot of autonomy, there‚Äôs room for impactful infrastructure work, and I‚Äôm helping shape how data is handled org-wide. We use a fairly modern stack: Snowflake, Databricks, Airflow, AWS, etc.

A technical recruiter from Meta recently reached out to say they‚Äôre hiring DEs (L4/L5) and invited me to begin technical interviews.

I‚Äôm torn on what decision would be best for my career: Should I pursue the opportunity at Meta, or stay in my current role and keep building?

Here are some things I‚Äôm weighing:

* Prestige: Having work experience at a company like Meta could open doors for me in the future.
* Tech stack: I‚Äôve heard Meta uses mostly in-house tools (some open sourced), and I worry that might hurt future job transitions where industry-standard tools are more relevant.
* Role scope: I‚Äôve read that DEs at Meta may do work closer to analytics engineering. I enjoy analytics, but I‚Äôd miss the more technical DE aspects.
* Compensation: I‚Äôm currently making \~$160K base + pre-IPO equity + bonus potential. Meta‚Äôs base range is similar, but equity would likely be more valuable and far lower risk.
* Location: My current role is entirely remote. I would have to relocate to accommodate Meta's hybrid in person requirement.

So if you were in my shoes, what would you do? I appreciate any thoughts or advice!",7,10,indyscout,1745194007.0,https://www.reddit.com/r/dataengineering/comments/1k408vu/seeking_advice_is_de_at_meta_worth_pursuing/,0.67,False,1745194305.0,False,False
1k41hcg,Cloudflare R2 + Apache Iceberg + R2 Data Catalog + Daft,,7,0,averageflatlanders,1745197838.0,https://dataengineeringcentral.substack.com/p/cloudflare-r2-apache-iceberg-r2-data,0.9,False,False,False,False
1k3wpp9,I've been testing LLMs for data transformations and results have been great,"There are two main reasons why I've been testing this. First, in scenarios where you have hundreds of different data sources each with similar data but varying schemas, doing transformations with an LLM would mean you don't have to write hundreds of different transformation processes. manage all of them etc. Additionally, when the those sources inevitably alter their schemas slightly, you don't have to worry about your rigid transformation processes breaking.

The next use case I had in mind was enriching the data by using the LLM to make inferences that would be time-consuming or even impossible to do with traditional code. For simple example, I had a field that contained mix of individual and business names. Some of my sources included a field that indicated the entity type, others did not. I found that the LLM was very accurate not only for determining whether the entity was an individual or not, but also ignoring the records that did have this indicator already. I've also tested more complex inference logic with similarly accurate results.

 I was able to build a single prompt that does several transformations and inferences all at the same time, receiving validated structured output  from the LLM. From there, the data goes through a more traditional SQL transformation process.

I really thought there would be more issues with hallucination, but so far that just hasn't been the case.  The only inaccuracies I've found were in edge cases that would have caused issues with traditional transformations as well. To be fair, I'm using context amounts that are much, much smaller than the models are supposedly capable of dealing with and I suspect if I increased the context I would start to see issues. 

I first did some limited testing on this over a year ago, and while I remember being surprised then by how well it worked, the cost made it viable for only small datasets. I just thought it was a neat trick and didn't give it much more thought. But now the models are 20x cheaper in some cases. They are cheap enough now that I can run the same prompt through multiple models and flag anytime they disagree, which is almost always tends to be edge cases when both models were confused because the data itself had issues.

I'm wondering if anyone else has tested similar processes and, if so, how did your results look? I know my use case may be niche, but I have to think this approach is going to gain popularity as these models get cheaper and more capable over the years. ",6,3,arctic_radar,1745183686.0,https://www.reddit.com/r/dataengineering/comments/1k3wpp9/ive_been_testing_llms_for_data_transformations/,0.72,False,False,False,False
1k3ugrj,Looking for recent trends or tools to explore in the data world,"Hey everyone,

 I'm currently working on strengthening my tech watch efforts around the data ecosystem and I‚Äôm looking for fresh ideas on recent features, tools, or trends worth diving into.

Any suggestions are welcome ‚Äî thanks in advance!",4,2,Ilyes_ch,1745177352.0,https://www.reddit.com/r/dataengineering/comments/1k3ugrj/looking_for_recent_trends_or_tools_to_explore_in/,0.76,False,False,False,False
1k3u13r,My first on-cloud data engineering project,"I have done these two projects:

  
**Real Time Azure Data Lakehouse Pipeline (Netflix Analytics) | Databricks, Synapse Mar. 2025**

‚Ä¢ Delivered a real time medallion architecture using Azure data factory, Databricks, Synapse, and Power BI.

‚Ä¢ Built parameterized ADF pipelines to extract structured data from GitHub and ADLSg2 via REST APIs, with

validation and schema checks.

‚Ä¢ Landed raw data into bronze using auto loader with schema inference, fault tolerance, and incremental loading.

‚Ä¢ Transformed data into silver and gold layers using modular PySpark and Delta Live Tables with schema evolution.

‚Ä¢ Orchestrated Databricks Workflows with parameterized notebooks, conditional logic, and error handling.

‚Ä¢ Implemented CI/CD to automate deployment of notebooks, pipelines, and configuration across environments.

‚Ä¢ Integrated with Synapse and Power BI for real-time analytics with 100% uptime during validation.

**Enterprise Sales Data Warehouse | SQL¬∑ Data Modeling¬∑ ETL/ELT¬∑ Data Quality¬∑ Git Apr. 2025**

‚Ä¢ Designed and delivered a complete medallion architecture (bronze, silver, gold) using SQL over a 14 days.

‚Ä¢ Ingested raw CRM and ERP data from CSVs (>100KB) into bronze with truncate plus insert batch ELT,

achieving 100% record completeness on first run.

‚Ä¢ Standardized naming for 50+ schemas, tables, and columns using snake case, resulting in zero naming conflicts across 20 Git tracked commits.

‚Ä¢ Applied rule based quality checks (nulls, types, outliers) and statistical imputation resulting in 0 defects.

‚Ä¢ Modeled star schema fact and dimension tables in gold, powering clean, business aligned KPIs and aggregations.

‚Ä¢ Documented data dictionary, ER diagrams, and data flow



**QUESTION: What would be a step up from this now?**   
**I think I want to focus on Azure Data Engineering solutions.** ",5,1,Gloomy-Profession-19,1745176141.0,https://www.reddit.com/r/dataengineering/comments/1k3u13r/my_first_oncloud_data_engineering_project/,0.78,False,False,False,False
1k3pzh7,Feedback on my MCD for a training management system?,"Hey everyone! üëã

I‚Äôm working on a¬†**Conceptual Data Model (MCD)**¬†for a training management system and I‚Äôd love to get some feedback

The main elements of the system are:

* **Formateurs**¬†(trainers) teach¬†**Modules**
* Each¬†**Module**¬†is scheduled into one or more¬†**S√©ances**¬†(sessions)
* **Stagiaires**¬†(trainees) can participate in sessions, and their participation can be marked as ""Present"" or ""Absent""
* If a trainee is absent, there can be a¬†**Justification**¬†linked to that absence

I decided to merge the ""Assistance"" (Assister) and ‚ÄúAbsence‚Äù (Absenter) relationships into a single¬†**Participation**¬†relationship with a possible attribute like¬†`Status`, and added a link from participation to a¬†**Justification**¬†(0 or 1).

Does this structure look correct to you? Any suggestions to improve the logic, simplify it further, or potential pitfalls I should watch out for?

Thanks in advance for your help

https://preview.redd.it/mpn8p43kk0we1.png?width=806&format=png&auto=webp&s=ea0c00e582b94a8168a3d991d90cef5439c3bee9

",6,0,drawlin__,1745165359.0,https://www.reddit.com/r/dataengineering/comments/1k3pzh7/feedback_on_my_mcd_for_a_training_management/,1.0,False,False,False,False
1k3pnjf,How do you balance short and long term as an IC,"Hi all ! I'm an analytics engineer not DE but felt it would be relevant to ask this here.

When you're taking on a new project, how do you think about balancing turning something around asap vs really digging in and understanding and possibly delivering something better? 

For example, I have a report I'm updating and adding to. On one extreme, I could probably ship the thing in like a week without much of an understanding outside of what's absolutely necessary to understand to add what needs to be added. 

On the other hand, I could pull the thread and work my way all the way from source system to queries that create the views to the transformations done in the reporting layer and understanding the business process and possibly modeling the data if that's not already done etc 

I know oftentimes I hear leaders of data teams talk about balancing short versus long-term investments, but even as an IC I wonder how y'all do it?

In a previous role, I aired on the side of understanding everything super deeply from the ground up on every project, but that means you don't deliver things quickly.",5,7,0sergio-hash,1745164488.0,https://www.reddit.com/r/dataengineering/comments/1k3pnjf/how_do_you_balance_short_and_long_term_as_an_ic/,0.73,False,False,False,False
1k44izr,Most prominent data quality issues,"Hello,

For those expert in the field or has been in the field for 5 years and more, what you would say are top issues you face when it comes to data quality and observability in snowflake?

",3,0,Existing-Push-2142,1745207790.0,https://www.reddit.com/r/dataengineering/comments/1k44izr/most_prominent_data_quality_issues/,1.0,False,False,False,False
1k3yg6y,Career Advice,"I have been working as a Data Analyst in my company for the last 6 years. I feel that I have become stagnant in my role and looking to break into a DE role in other teams to up-skill and get better pay as I have been doing some DE work recently. However, I am closer to a promotion in my current role but not sure when it will happen. If I move to a DE role at same level my promotion will be delayed.

Should I wait it out and get a promotion in my current role or start looking into transitioning to DE roles in other teams?
",2,2,TastyBrilliant5132,1745188652.0,https://www.reddit.com/r/dataengineering/comments/1k3yg6y/career_advice/,0.67,False,False,False,False
1k3wenv,Need advice: Codec (Data Engineer) vs Optum (Data Analyst) offer ‚Äî which one to choose?,"Hi everyone,

I‚Äôve just received two job offers ‚Äî one from Codec for a Data Engineer role and another from Optum for a Data Analyst position. I'm feeling a bit confused about which one to go with.

Can anyone share insights on the roles or the companies that might help me decide? I'm especially curious about growth opportunities, work-life balance, and long-term career prospects in each.

Would love to hear your thoughts on:

Company culture and work-life balance

Tech stack and learning opportunities

Long-term prospects in Data Engineer vs Data Analyst roles at these companies

Thanks in advance for your help!",2,5,FluffyBonus3868,1745182834.0,https://www.reddit.com/r/dataengineering/comments/1k3wenv/need_advice_codec_data_engineer_vs_optum_data/,0.59,False,False,False,False
1k3n8h9,Live CSV updating,"Hi everyone , 

I have a software that writes live data to a CSV file in realtime. I want to be able to import this data every second, into Excel or a another spreadsheet program, where I can use formulas to mirror cells and manipulate my data. I then want this to export to another live CSV file in realtime. Is there any easy way to do this? 

I have tried Google sheets (works for json but not local CSV, and requires manual updates)

I have used macros in VBA in excel to save and refresh data every second and it is unreliable. 

Any help much appreciated.. possibly create a database?",5,6,adamgmx24,1745157782.0,https://www.reddit.com/r/dataengineering/comments/1k3n8h9/live_csv_updating/,0.78,False,False,False,False
1k471tz,How can I speed up the Stream Buffering in BigQuery?,"Hello all, I have created a backfill for a table which is about 1gb and tho the backfill finished very quickly, I am still having problems querying the database as the data is in buffering (Stream Buffer). How can I speed up the buffering and make sure the data is ready to query? 

Also, when I query the data sometimes I get the query results and sometimes I don't (same query), this is happening randomly, why is this happening?

P.S., We usually change the staleness limit to 5 mins, now sure what effect this has on the buffering tho, my rationale is, since the data is considered to be so outdated, it will get a priority in system resources when it comes to buffering. But, is there anything else we can do?

",1,3,Weird-Trifle-6310,1745217691.0,https://www.reddit.com/r/dataengineering/comments/1k471tz/how_can_i_speed_up_the_stream_buffering_in/,1.0,False,1745217881.0,False,False
1k46wz7,How can I capture deletes in CDC if I can't modify the source system?,"I'm working on building a data pipeline where I need to implement Change Data Capture (CDC), but I don't have permission to modify the source system at all ‚Äî no schema changes (like adding `is_deleted` flags), no triggers, and no access to transaction logs.

I still need to detect **deletes** from the source system. Inserts and updates are already handled through timestamp-based extracts.

Are there best practices or workarounds others use in this situation?

So far, I found that comparing primary keys between the source extract and the warehouse table can help detect missing (i.e., deleted) rows, and then I can mark those in the warehouse. Are there other patterns, tools, or strategies that have worked well for you in similar setups?

For context:

* Source system = \[insert your DB or system here, e.g., PostgreSQL used by Odoo\]
* I'm doing periodic batch loads (daily).
* I use \[tool or language you're using, e.g., Python/SQL/Apache NiFi/etc.\] for ETL.

Any help or advice would be much appreciated!",2,1,Acceptable-Ride9976,1745217102.0,https://www.reddit.com/r/dataengineering/comments/1k46wz7/how_can_i_capture_deletes_in_cdc_if_i_cant_modify/,1.0,False,False,False,False
1k45ujt,Anyone attending the Databricks Field Lab in London on April 29?,"Hey everyone, Databricks and Datapao are running a free Field Lab in London on April 29. It‚Äôs a full-day, hands-on session where you‚Äôll build an end-to-end data pipeline using streaming, Unity Catalog, DLT, observability tools, and even a bit of GenAI + dashboards. It‚Äôs very practical, lots of code-along and real examples. Great if you're using or exploring Databricks. [https://events.databricks.com/Datapao-Field-Lab-April](https://events.databricks.com/Datapao-Field-Lab-April) 

",1,1,Adept_Explanation831,1745212655.0,https://www.reddit.com/r/dataengineering/comments/1k45ujt/anyone_attending_the_databricks_field_lab_in/,1.0,False,False,False,False
1k41dxp,(Streaming) How do you know if things are complete ?,"I didn‚Äôt work a lot with streaming concept, did mostly batch.

I‚Äôm wondering how do you define when a data will be done?

For example you count the sums of multiple blockchain wallets. You have the transactions and end up doing sum over a time period. Let‚Äôs say you do this per 15 min periods. How do you know you period is finished ? Like you define that arbitrary like 30min and hope for the best ?

Can you reprocess the same period later if some system fail badly ?  

I except a very generic answer here. I just don‚Äôt understand the concept. Like do you need to have data that if you miss some records it‚Äôs fine to deliver Half the response or can you have precise data there too where every records count ?

TLDR; how do you validate that you have all your data before letting the downstream module consume an aggregated topic or flush the period of aggregation from the stream ?",2,1,Commercial_Dig2401,1745197540.0,https://www.reddit.com/r/dataengineering/comments/1k41dxp/streaming_how_do_you_know_if_things_are_complete/,0.67,False,False,False,False
1k3yb5c,Spark JDBC datasource,"Is it just me or is the Spark JDBC datasource really not designed to deal with large volumes of data?  All I want to do is read a table from Microsoft SQL Server and write it out as parquet files.  The table has about 200 million rows.  If I try to run this without using a JDBC partitionColumn, the node that is pulling the data just runs out of memory and disk space.  If I add a partitionColumn and several partitions, Spark can spread the data pull out over several nodes, but it opens a whole bunch of concurrent connections to the DB.  For obvious reasons I don't want to do something like open 20 concurrent connections to a production database.  I already bumped up the number of concurrent connections to 12 and some nodes are still running out of memory, probably because the data is not evenly distributed by the partition column.

I also ran into cases where the Spark job would pull all the partitions from the same executor, which makes no sense.  This JDBC datasource thing seems severely limited unless I'm overlooking something.  Are there any Spark users who do this regularly and have tips?  I am considering just using another tool like Sqoop.",2,5,doobiedoobie123456,1745188230.0,https://www.reddit.com/r/dataengineering/comments/1k3yb5c/spark_jdbc_datasource/,0.67,False,False,False,False
1k3zle1,Which companies outside of FAANG make $200k+ for DE?,"For a Senior DE, which companies have a relevant tech stack, pay well, and have decent WLB outside of FAANG? ",1,16,TownAny8165,1745192055.0,https://www.reddit.com/r/dataengineering/comments/1k3zle1/which_companies_outside_of_faang_make_200k_for_de/,0.52,False,False,False,False
